# Nova Memory Configuration Example
# Copy this file to config.toml and customize for your deployment

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
# Nova Memory uses a three-tier storage system:
# - Hot: In-memory cache for fastest access (Redis-like)
# - Warm: Local disk storage for moderate access (SQLite/Files)
# - Cold: Cloud/offsite backup for archival (S3-compatible)

[storage]
# Hot cache size in GB (in-memory, fastest access)
# Default: 10
hot_cache_gb = 10

# Warm storage size in GB (local disk, moderate access)
# Default: 50
warm_storage_gb = 50

# Enable cold storage tier (cloud/offsite backup)
# Default: true
cold_enabled = true

# Base directory for all storage data
# Default: ~/.nova-memory
data_dir = "~/.nova-memory"

# =============================================================================
# PROXY CONFIGURATION
# =============================================================================
# The HTTP proxy sits between your application and the LLM API,
# injecting relevant memories into each request.

[proxy]
# Address to listen on for incoming requests
# Default: 127.0.0.1:9999
listen_addr = "127.0.0.1:9999"

# Upstream LLM API URL (REQUIRED)
# This is the actual LLM API endpoint that requests will be forwarded to
# Examples:
#   - OpenAI: https://api.openai.com/v1
#   - Anthropic: https://api.anthropic.com/v1
#   - Local: http://localhost:11434/v1
upstream_url = "https://api.openai.com/v1"

# Request timeout in seconds
# Default: 300 (5 minutes)
timeout_secs = 300

# Maximum tokens to inject into context window
# This limits how much memory content is added to each request
# Default: 2000
max_injection_tokens = 2000

# =============================================================================
# ROUTER CONFIGURATION
# =============================================================================
# The router determines which memories are relevant to each request
# and how they should be retrieved.

[router]
# Strategy for selecting relevant memories
# Options: "semantic", "keyword", "hybrid"
# Default: "semantic"
strategy = "semantic"

# Maximum number of memories to retrieve per request
# Default: 10
max_memories = 10

# Minimum relevance score threshold (0.0 to 1.0)
# Memories below this score will not be included
# Default: 0.7
relevance_threshold = 0.7

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================
# Configuration for the embedding model used to convert text
# into vector representations for semantic search.

[embedding]
# Embedding model provider
# Options: "openai", "anthropic", "local", "huggingface"
# Default: "openai"
provider = "openai"

# Model name or identifier
# Examples:
#   - OpenAI: "text-embedding-3-small", "text-embedding-3-large"
#   - Local: "sentence-transformers/all-MiniLM-L6-v2"
# Default: "text-embedding-3-small"
model = "text-embedding-3-small"

# Embedding dimension size
# Must match the output dimension of your chosen model
# Default: 1536 (for text-embedding-3-small)
dimension = 1536

# Batch size for embedding generation
# Larger batches are more efficient but use more memory
# Default: 32
batch_size = 32
