# Mnemo Configuration Example
# Copy this file to config.toml and customize for your deployment

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================
# Mnemo uses a three-tier storage system:
# - Hot: In-memory cache for fastest access (Redis-like)
# - Warm: Local disk storage for moderate access (SQLite/Files)
# - Cold: Cloud/offsite backup for archival (S3-compatible)

[storage]
# Hot cache size in GB (in-memory, fastest access)
# Default: 10
hot_cache_gb = 10

# Warm storage size in GB (local disk, moderate access)
# Default: 50
warm_storage_gb = 50

# Enable cold storage tier (cloud/offsite backup)
# Default: true
cold_enabled = true

# Base directory for all storage data
# Default: ~/.mnemo
data_dir = "~/.mnemo"

# =============================================================================
# PROXY CONFIGURATION
# =============================================================================
# The HTTP proxy sits between your application and the LLM API,
# injecting relevant memories into each request.

[proxy]
# Address to listen on for incoming requests
# Default: 127.0.0.1:9999
listen_addr = "127.0.0.1:9999"

# Upstream LLM API URL (OPTIONAL)
# This is the default LLM API endpoint that requests will be forwarded to.
# When using dynamic passthrough (/p/{url} routes), this can be omitted.
# If set, requests not matching /p/* will be forwarded to this URL.
# Examples:
#   - OpenAI: https://api.openai.com/v1
#   - Anthropic: https://api.anthropic.com/v1
#   - Local: http://localhost:11434/v1
# upstream_url = "https://api.openai.com/v1"

# Host allowlist for dynamic passthrough security
# - Empty list allows ALL hosts (not recommended for production)
# - Exact matches: "api.openai.com"
# - Wildcard subdomains: "*.anthropic.com" (matches api.anthropic.com, etc.)
#
# SECURITY WARNING: Without allowed_hosts, the proxy can be used to access
# any HTTP(S) endpoint, potentially bypassing firewalls.
allowed_hosts = [
    "api.openai.com",
    "api.anthropic.com",
    "*.groq.com",
]

# Dynamic passthrough examples:
#   curl http://localhost:9999/p/https://api.openai.com/v1/chat/completions
#   curl http://localhost:9999/p/https://api.anthropic.com/v1/messages

# Request timeout in seconds
# Default: 300 (5 minutes)
timeout_secs = 300

# Maximum tokens to inject into context window
# This limits how much memory content is added to each request
# Default: 2000
max_injection_tokens = 2000

# =============================================================================
# ROUTER CONFIGURATION
# =============================================================================
# The router determines which memories are relevant to each request
# and how they should be retrieved.

[router]
# Strategy for selecting relevant memories
# Options: "semantic", "keyword", "hybrid"
# Default: "semantic"
strategy = "semantic"

# Maximum number of memories to retrieve per request
# Default: 10
max_memories = 10

# Minimum relevance score threshold (0.0 to 1.0)
# Memories below this score will not be included
# Default: 0.7
relevance_threshold = 0.7

# -----------------------------------------------------------------------------
# DETERMINISTIC RETRIEVAL (for improved LLM cache hit rates)
# -----------------------------------------------------------------------------
# When enabled, similar queries produce identical memory orderings,
# improving cache efficiency with providers like Anthropic that cache
# based on prompt prefixes.

[router.deterministic]
# Enable deterministic memory retrieval ordering
# Default: false
enabled = false

# Score quantization precision (decimal places 1-4)
# Lower values = more cache hits but less precision
# Default: 2
decimal_places = 2

# Weight for topic/entity overlap in scoring (0.0-1.0)
# Memories matching query entities get boosted by this factor
# Default: 0.1
topic_overlap_weight = 0.1

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================
# Configuration for the embedding model used to convert text
# into vector representations for semantic search.

[embedding]
# Embedding model provider
# Options: "openai", "anthropic", "local", "huggingface"
# Default: "openai"
provider = "openai"

# Model name or identifier
# Examples:
#   - OpenAI: "text-embedding-3-small", "text-embedding-3-large"
#   - Local: "sentence-transformers/all-MiniLM-L6-v2"
# Default: "text-embedding-3-small"
model = "text-embedding-3-small"

# Embedding dimension size
# Must match the output dimension of your chosen model
# Default: 1536 (for text-embedding-3-small)
dimension = 1536

# Batch size for embedding generation
# Larger batches are more efficient but use more memory
# Default: 32
batch_size = 32

# =============================================================================
# CURATOR CONFIGURATION
# =============================================================================
# The memory curator uses an LLM to analyze conversations and create
# high-quality semantic memories. It can run locally (default) or use
# a remote API for curation.

[curator]
# Enable the memory curator
# Default: false
enabled = false

# Provider type: local, remote, or hybrid
# Default: local
provider = "local"

# -----------------------------------------------------------------------------
# LOCAL LLM CONFIGURATION (when provider = "local")
# -----------------------------------------------------------------------------
[curator.local]
# Model ID from HuggingFace or local path
# Default: Qwen/Qwen3-1.7B (small, fast, good quality)
model_id = "Qwen/Qwen3-1.7B"

# Quantization level for local model
# Options: Q4K, Q5K, Q6K, Q8, F16, F32
# Default: Q4K (good balance of speed and quality)
quantization = "Q4K"

# Use GPU acceleration if available
# Default: false (CPU is safer default)
use_gpu = false

# Maximum context length in tokens
# Default: 4096
context_length = 4096

# -----------------------------------------------------------------------------
# REMOTE API CONFIGURATION (when provider = "remote" or "hybrid")
# -----------------------------------------------------------------------------
[curator.remote]
# API endpoint URL (required for remote provider)
# Examples:
#   - OpenAI: https://api.openai.com/v1/chat/completions
#   - Anthropic: https://api.anthropic.com/v1/messages
# Default: (empty)
api_url = ""

# Environment variable name for API key
# Default: CURATOR_API_KEY
api_key_env = "CURATOR_API_KEY"

# Model identifier for remote API
# Default: gpt-4o-mini (cost-effective, good quality)
model = "gpt-4o-mini"

# Request timeout in seconds
# Default: 30
timeout_secs = 30

# -----------------------------------------------------------------------------
# CONVERSATION BUFFER CONFIGURATION
# -----------------------------------------------------------------------------
[curator.buffer]
# Maximum number of conversation turns to buffer before curation
# Default: 10
max_turns = 10

# Maximum tokens in buffer before triggering curation
# Default: 8000
max_tokens = 8000

# -----------------------------------------------------------------------------
# INJECTION TRACKING CONFIGURATION
# -----------------------------------------------------------------------------
[curator.injection_tracking]
# Enable tracking of which injected memories were actually used
# This creates a feedback loop to improve memory quality
# Default: true
enabled = true

# Maximum number of tracked injections to store
# Default: 1000
max_entries = 1000

# Penalty factor for memories that were injected but not referenced
# Range: 0.0-1.0 (higher = more aggressive penalty)
# Default: 0.3
penalty_factor = 0.3
